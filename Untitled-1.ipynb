{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PytorchPipeline' from 'pytorch_pipeline' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/37/0vzj0mmd5tj4y1hrgj2ls_kc0000gn/T/ipykernel_2473/2193868255.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./pytorch_pipeline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_pipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPytorchPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PytorchPipeline' from 'pytorch_pipeline' (unknown location)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./dataset')\n",
    "from dataset import *\n",
    "\n",
    "sys.path.append('./pytorch_pipeline')\n",
    "from ptpl import PytorchPipeline\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchdyn.models import *\n",
    "# from torchdyn.datasets import *\n",
    "from torchdyn import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            emb_dim,\n",
    "            hidden_dim,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, emb_dim)\n",
    "        Returns: \n",
    "            hidden: Hidden tensor of shape (1, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        x = x.transpose(1, 0) # transform to (seq_len, batch_size, emb_dim)\n",
    "        _, hidden = self.rnn(self.dropout(x))\n",
    "        return hidden\n",
    "\n",
    "# Define decoder \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,  hidden_dim, vocab_size, device, nfe):\n",
    "        super(Decoder, self).__init__()\n",
    "        func = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.ode_solve = NeuralODE(\n",
    "            func, \n",
    "            sensitivity = 'autograd', \n",
    "            solver = 'rk4', \n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, hidden, t_span):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden: Hidden vector of shape (1, batch_size, hidden_dim).\n",
    "                    It is assumed that the encoder is one directional and has only one layer\n",
    "            t_span: Interval to evaluate the ODE.\n",
    "                    Should be a tensor of shape (num_eval_points).\n",
    "                    It is advised to initialize as follows:\n",
    "                        t_span = torch.linspace(0, 1, num_eval_points)\n",
    "        Returns:\n",
    "            out: Output tensor of shape (num_eval_points, batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden = hidden.squeeze(0)\n",
    "        trat_eval, trajectory = self.ode_solve(hidden, t_span)\n",
    "        out = self.fc(self.dropout(trajectory))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, embedding, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.embedding = embedding\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x, t_span):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input sequence tensor of shape (batch_size, seq_len)\n",
    "            t_span: Interval to evaluate the ODE.\n",
    "                    Should be a tensor of shape (num_eval_points).\n",
    "                    It is advised to initialize as follows:\n",
    "                        t_span = torch.linspace(0, 1, num_eval_points).\n",
    "        Returns:\n",
    "            out: Output tensor of shape (num_eval_points, batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        hidden = self.encoder(x)\n",
    "        out = self.decoder(hidden, t_span)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences is 135842 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135842/135842 [00:00<00:00, 511838.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size if 5384 \n",
      "\n",
      "The size of the newly created text data is 122885, 90% of the original text\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(\"./data\", \"eng-fra.txt\")\n",
    "perSentence, fullVocab = get_loaders(path)\n",
    "dataset =  ds(perSentence[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your vector field callable (nn.Module) should have both time `t` and state `x` as arguments, we've wrapped it for you.\n"
     ]
    }
   ],
   "source": [
    "num_batches = 2\n",
    "seq_length = 6\n",
    "vocab_size = len(fullVocab)\n",
    "span = 50\n",
    "nfe = 20\n",
    "hidden_dim = 64\n",
    "emb_dim = 32\n",
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "encoder = Encoder(emb_dim, hidden_dim)\n",
    "decoder = Decoder(hidden_dim, vocab_size, device, nfe)\n",
    "\n",
    "model = Seq2Seq(embedding, encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PyTorchPipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/37/0vzj0mmd5tj4y1hrgj2ls_kc0000gn/T/ipykernel_2473/3082262407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m ptpl = PyTorchPipeline(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mproject_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gru_node\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     configs = {\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PyTorchPipeline' is not defined"
     ]
    }
   ],
   "source": [
    "hparams = {\n",
    "    'wb' : False,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'emb_dim': emb_dim,\n",
    "    'nfe': nfe,\n",
    "    'num_epochs': 100,\n",
    "    'num_batches' : 32,\n",
    "    'path2save': \"./weights/hidden_size_\" + str(hidden_dim) + \"_emb_dim_\" + str(emb_dim) + \"_nfe_\" + str(nfe) + \".pt\",\n",
    "    'learning_rate': 1e-3,\n",
    "}\n",
    "\n",
    "ptpl = PyTorchPipeline(\n",
    "    project_name = \"gru_node\",\n",
    "    configs = {\n",
    "        'device': device,\n",
    "        'criterion': criterion,\n",
    "        'optimizer': optimizer,\n",
    "        'train_dataloader': trainPerSentence,\n",
    "        'val_dataloader': valPerSentence,\n",
    "        'print_logs': True,\n",
    "        'wb': hparams['wb'],\n",
    "    },\n",
    "    hparams = hparams,\n",
    "    model = model,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset,\n",
    "    batch_size= num_batches,\n",
    "    num_workers= 0,\n",
    "    shuffle = False,\n",
    ")\n",
    "\n",
    "x = next(iter(dataloader))\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    t_span = torch.linspace(0,20, 6)\n",
    "    y = model(x, t_span)\n",
    "    y = y.transpose(1,0)\n",
    "    \n",
    "\n",
    "    y_flattened = y.reshape(-1, y.shape[-1])\n",
    "    x_flattened = x.reshape(-1)\n",
    "\n",
    "    loss = criterion(y_flattened, x_flattened)\n",
    "    loss.backward()\n",
    "    print(loss.item())\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0975cbd9b939ec45f50e3d264ec9b47d57a929f5808edd8ce4f4fa6b6568457"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
